import numpy as np

def f(x1, x2):
    return x1 + 2 * x2 + 4 * np.sqrt(1 + x1**2 + x2**2)

def grad_f(x1, x2):
    common_term = 4 / np.sqrt(1 + x1**2 + x2**2)
    df_dx1 = 1 + x1 * common_term
    df_dx2 = 2 + x2 * common_term
    return np.array([df_dx1, df_dx2])

def steepest_descent(f, grad_f, initial_x, step_size=0.01, tol=1e-6, max_iters=1000):
    x = np.array(initial_x)
    for i in range(max_iters):
        grad = grad_f(x[0], x[1])
        x_new = x - step_size * grad
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new
    return x, f(x[0], x[1]), i

initial_x = [0, 0]
optimal_x, optimal_value, iterations = steepest_descent(f, grad_f, initial_x)

print(f"Точка оптимума: ({optimal_x[0]:.6f}, {optimal_x[1]:.6f})")
print(f"Значение оптимума: {optimal_value:.6f}")
